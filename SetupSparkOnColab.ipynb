{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SetupSparkOnColab.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOnKTM01u17NywIzZsXCH52"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Spark on Colab Notebooks\n",
        "\n",
        "The notebook explains how to setup spark on Colab Notebooks. PySpark is available via pip as well, which should be the way, but this is also useful to know :)\n",
        "\n"
      ],
      "metadata": {
        "id": "SxVRsOsQP9CG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup JVM"
      ],
      "metadata": {
        "id": "zXDPU9mKQU8Q"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "id": "YLTDSslmNRth"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download chosen version of Spark from the website\n",
        "\n",
        "# Refer https://spark.apache.org/downloads.html for more details!\n",
        "# Naturally, in case new version is used, it would have to specified in subsequent commands"
      ],
      "metadata": {
        "id": "dbEds03gQW43"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "rJv1FAU_ODWi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip Spark folder"
      ],
      "metadata": {
        "id": "gLyL_mbfQoGg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "FF2hGKrdNfxX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install FindSpark to locate Spark and import it as a regular Python library"
      ],
      "metadata": {
        "id": "SSqU582qQqyU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "tcUgC4wQNium"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Env variables"
      ],
      "metadata": {
        "id": "rEz0r_qbQ3dT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "IC2CIsS1MHR-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize findspark"
      ],
      "metadata": {
        "id": "VHxdOKJdQ5zS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "SLBm2fumOm_n"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In case you need to locate Spark Bin, use .find()"
      ],
      "metadata": {
        "id": "vml0Xex5Q84Y"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "findspark.find()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OB01zRObOp-D",
        "outputId": "56f62b72-970e-4472-ba7e-75d9f73e32b9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/spark-3.2.1-bin-hadoop3.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports should work fine now!"
      ],
      "metadata": {
        "id": "_2swElBORB0j"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql.types import LongType\n",
        "print(\"Imports successful!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZ6QCWKmMGD2",
        "outputId": "4021b7b7-d06a-4f3c-946f-30dcc9a6aeee"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imports successful!\n"
          ]
        }
      ]
    }
  ]
}